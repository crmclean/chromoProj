library(here)
path2data <- here("data/2018-04-23/rds_files/")
list.files(path2data)
file.path(paht2data, list.files(path2data))
path2data <- here("data/2018-04-23/rds_files/")
file.path(path2data, list.files(path2data))
datapath <- file.path(path2data, list.files(path2data))
readRDS(datapath[1])
## trying out no FP
autoTuner_out <- readRDS(datapath[2])
library(here)
path2data <- here("data/2018-04-23/rds_files/")
datapath <- file.path(path2data, list.files(path2data))
## trying out no FP
autoTuner_out <- readRDS(datapath[2])
autoTuner_out
library(here)
path2data <- here("data/2018-04-23/rds_files/")
datapath <- file.path(path2data, list.files(path2data))
## trying out no FP
xgF <- readRDS(datapath[2])
xsa<-xsAnnotate(xgF)
source("https://bioconductor.org/biocLite.R")
biocLite("CAMERA")
library(CAMERA)
library(here)
path2data <- here("data/2018-04-23/rds_files/")
datapath <- file.path(path2data, list.files(path2data))
## trying out no FP
xgF <- readRDS(datapath[2])
xsa<-xsAnnotate(xgF)
#group the features initially just by retention time
xsaF <-groupFWHM(xsa)
#figure out which features also have a matching 13C feature. Have to enter both
#the relative error (ppm) and the absolute error (mzabs)
xsaFI <-findIsotopes(xsaF,ppm=15,mzabs = 0.001,minfrac = 1/nSamples)
#figure out which features also have a matching 13C feature. Have to enter both
#the relative error (ppm) and the absolute error (mzabs)
xsaFI <-findIsotopes(xsaF,ppm=15,mzabs = 0.001,minfrac = 1/6)
#now group by the correlations based on (1) intensity, (2) EIC, (3) isotopes...
xsaC <-groupCorr(xsaFI,cor_eic_th=0.75,pval=0.05, graphMethod="hcs",
calcIso = TRUE, calcCiS = TRUE, calcCaS = FALSE)
#setup the file to also look for adducts, only go with the primary adducts for the moment
rules <-read.csv(system.file("rules/primary_adducts_neg.csv",package = "CAMERA"))
an <-findAdducts(xsaC,polarity = "negative",rules=rules,ppm=15)
a
an
outputPath <- file.path(path2data, "chromo_cam_noFP.csv")
outputPath
write.csv(getPeaklist(an),file=outputPath)
getPeaklist(an)
data <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
data
data %>% head()
# Loading packages --------------------------------------------------------
library(dplyr)
data %>% head()
dataObservations
# Loading packages --------------------------------------------------------
library(dplyr)
library(here)
library(limma)
runfile <- read.csv(here("data/CROMO_11_metadata.csv"), stringsAsFactors = F)
data <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
# QA on data --------------------------------------------------------------
dataCols <- grep(colId, colnames(data))
blankMatches <- colnames(data) %in% runfile$Sample.Name[runfile$Well %in% "Blank"]
blankCols <- colnames(data)[blankMatches]
colId <- sub("_00[0-9]", "", blankCols[1])
for(curCol in blankCols) {
## determining type of blank
blankCol <- data[,colnames(data) %in% curCol]
removeRows <- which(!is.na(blankCol))
## determining which columns are same type as blank to correct
blankType <- runfile$Type[runfile$Sample.Name == curCol]
correctData <- colnames(data) %in% runfile$Sample.Name[runfile$Type %in% blankType]
## correcting the data
data[removeRows,correctData] <- NA
}
# counting total number of blanks observed within the data
removeRows <- data[,] %>% apply(1, function(row) {sum(is.na(row)) == length(row)})
message("Total rows removed through blank correction: ", sum(removeRows))
removeRows
sum(removeRows)
runfile <- read.csv(here("data/CROMO_11_metadata.csv"), stringsAsFactors = F)
data <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
# QA on data --------------------------------------------------------------
dataCols <- grep(colId, colnames(data))
blankMatches <- colnames(data) %in% runfile$Sample.Name[runfile$Well %in% "Blank"]
blankCols <- colnames(data)[blankMatches]
colId <- sub("_00[0-9]", "", blankCols[1])
dataCols
blankMatches
blankCols
colId
for(curCol in blankCols) {
## determining type of blank
blankCol <- data[,colnames(data) %in% curCol]
removeRows <- which(!is.na(blankCol))
## determining which columns are same type as blank to correct
blankType <- runfile$Type[runfile$Sample.Name == curCol]
correctData <- colnames(data) %in% runfile$Sample.Name[runfile$Type %in% blankType]
## correcting the data
data[removeRows,correctData] <- NA
}
# counting total number of blanks observed within the data
removeRows <- data[,] %>% apply(1, function(row) {sum(is.na(row)) == length(row)})
removeRows
sum(removeRows)
removeRows
correctData
blankMatches
dataCols
# counting total number of blanks observed within the data
removeRows <- data[,dataCols] %>% apply(1, function(row) {sum(is.na(row)) == length(row)})
removeRows
message("Total rows removed through blank correction: ", sum(removeRows))
# subsetting data to only consider sample columns
dataObservations <- data[,dataCols[!dataCols %in% which(blankMatches)]]
dataObservations <- apply(dataObservations, 2, function(col) {
col[is.na(col)] <- 0
return(col)
})
# removing rows considered to be adducts and isotopes
dataObservations
which(blankMatches)
!dataCols
dataCols
1:ncol(data) %in% which(blankMatches)
# subsetting data to only consider sample columns
dataObservations <- data[,!(1:ncol(data) %in% which(blankMatches))]
dataObservations
colId
dataObservations[colId ,grep(colnames(dataObservations))] <-
apply(dataObservations[colId ,grep(colnames(dataObservations))], 2, function(col) {
col[is.na(col)] <- 0
return(col)
})
colId
dataObservations[,grep(colId, colnames(dataObservations))] <-
apply(dataObservations[,grep(colId, colnames(dataObservations))], 2, function(col) {
col[is.na(col)] <- 0
return(col)
})
dataObservations
# removing rows considered to be adducts and isotopes
dataObservations$isotopes == ""
# removing rows considered to be adducts and isotopes
dataObservations$isotopes == ""  && dataObservations$adduct == ""
# removing rows considered to be adducts and isotopes
dataObservations$isotopes == ""  & dataObservations$adduct == ""
# removing rows considered to be adducts and isotopes
adduct_Isos <- dataObservations$isotopes == ""  & dataObservations$adduct == ""
message("Total rows removed following isotope/adduct corrections: ", sum(!adduct_Isos))
source('~/MIT/Research/chromoProj/scr/2018-04-19/data_viz/venn_diagram.R', echo=TRUE)
vennCountTable
dataObservations
dataObservations %>% head()
# venn diagram ------------------------------------------------------------
featureCounts <- dataObservations[,grep(colId, colnames(dataObservations))] %>%
apply(2, function(col) {col > 0})
featureCounts
vennCountTable <- vennCounts(featureCounts)
colnames(vennCountTable) <- c("CSW intracellular",
"CSW DOC",
"QV intracellular",
"QV DOC",
"Counts")
vennCountTable %>% head()
vennCountTable
vennDiagram(vennCountTable, circle.col = c("darkcyan", "darkblue", "darkorange", "darkorange4"),
mar = rep(0.01,4),
cex = c(1.2,1.2,1.2),
show.include = F)
# creating a density plot on the data -------------------------------------
grep(colId, colnames(data))
source('~/MIT/Research/chromoProj/scr/2018-04-20/dataViz/densityPlot.R', echo=TRUE)
# Loading metadata and real data ------------------------------------------
outpath <- here("Results/2018-04-23/densityPlot_camera.png")
runfile <- read.csv(here("data/CROMO_11_metadata.csv"), stringsAsFactors = F)
data <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
data
# QA on data --------------------------------------------------------------
dataCols <- grep(colId, colnames(data))
dataCols
blankMatches <- colnames(data) %in% runfile$Sample.Name[runfile$Well %in% "Blank"]
blankCols <- colnames(data)[blankMatches]
colId <- sub("_00[0-9]", "", blankCols[1])
for(curCol in blankCols) {
## determining type of blank
blankCol <- data[,colnames(data) %in% curCol]
removeRows <- which(!is.na(blankCol))
## determining which columns are same type as blank to correct
blankType <- runfile$Type[runfile$Sample.Name == curCol]
correctData <- colnames(data) %in% runfile$Sample.Name[runfile$Type %in% blankType]
## correcting the data
data[removeRows,correctData] <- NA
}
adduct_Isos <- data$isotopes == ""  & data$adduct == ""
data <- data[adduct_Isos,]
data
grep(colId, colnames(data))
grep(colId, colnames(data))
grep(colId, colnames(data))[-c(1,2)]
# creating a density plot on the data -------------------------------------
colnames(data)[grep(colId, colnames(data))[-c(1,2)]] <- c("CSW intracellular","CSW DOC", "QV intracellular", "QV DOC")
colnames(data)
meltedData <- reshape2::melt(data = data, id.vars = c("mz", "rt"), measure.vars = c("CSW intracellular",
"CSW DOC",
"QV intracellular",
"QV DOC"))
meltedDataSub <- meltedData[!is.na(meltedData$value),]
ggplot(meltedDataSub, mapping = aes(x = mz, y = rt, col = variable)) +
geom_point(size = .75, stroke = 0.25, shape = 16) +
theme_minimal() +
xlab("M/Z ratio") +
ylab("Retention Time (seconds)") +
ggtitle("Density Plot of Observed Features Across Samples") +
guides(colour = guide_legend(override.aes = list(size=3)))
source('~/MIT/Research/chromoProj/scr/2018-04-20/dataViz/densityPlot.R', echo=TRUE)
files <- list.files(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"), pattern = ".csv")
files
files <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"), pattern = ".csv")
files <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
files
peakTables <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
length(peakTables)
files
files
sub(".csv", "", files)
outPath
outPath <- "data/2018-04-23/mtabAnalyst_input/"
outPath
cpdPathTable
source('~/MIT/Research/chromoProj/scr/2018-04-18/kegg/checkKeggCpds.R', echo=TRUE)
cpdPathTable
cpdPathTable
cpdPathTable %>% head()
cpdName
cpdName
pathways
pathwayNames
pathwayNames
outputPath <- here("data/2018-04-23/keggTables/cpdMatches.csv")
write.table(cpdPathTable, file = outputPath, row.names = F)
keggTable <- read.csv(here("data/2018-04-23/keggTables/cpdMatches.csv"))
keggTable
keggTable %>% head()
cpdPathTable %>% head()
cpdPathTable %>% class()
write.csv(cpdPathTable, file = outputPath, row.names = F)
keggTable <- read.csv(here("data/2018-04-23/keggTables/cpdMatches.csv"))
keggTable
keggTable %>% head()
featureTable <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
featureTable %>% head()
# subsetting kegg table by metabolism -------------------------------------
lauren_mtabs <- c("methane metabolism", "carbon fixation pathways in prokaryotes",
"2-oxocarboxylic acid metabolism", "pentose phosphate pathway", "sulfur metabolism")
# set up ------------------------------------------------------------------
library(here)
library(dplyr)
keggTable <- read.csv(here("data/2018-04-23/keggTables/cpdMatches.csv"))
featureTable <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"))
# subsetting kegg table by metabolism -------------------------------------
lauren_mtabs <- c("methane metabolism", "carbon fixation pathways in prokaryotes",
"2-oxocarboxylic acid metabolism", "pentose phosphate pathway", "sulfur metabolism")
featureTable %>% head()
keggTable %>% head()
grep(currentMtab, keggTable$paths, ignore.case = T)
mtabIndex <- 1
curentMtab <- lauren_mtabs[mtabIndex]
grep(currentMtab, keggTable$paths, ignore.case = T)
currentMtab <- lauren_mtabs[mtabIndex]
grep(currentMtab, keggTable$paths, ignore.case = T)
keggTable[grep(currentMtab, keggTable$paths, ignore.case = T),]
curMtabTable <- keggTable[grep(currentMtab, keggTable$paths, ignore.case = T),]
split(curMtabTable, f = curMtabTable$Matched.Compound)
keggTable <- read.csv(here("data/2018-04-23/keggTables/cpdMatches.csv"), stringsAsFactors = F)
featureTable <- read.csv(here("data/2018-04-23/camera_tables/chromo_cam_noFP.csv"), stringsAsFactors = F)
# subsetting kegg table by metabolism -------------------------------------
lauren_mtabs <- c("methane metabolism", "carbon fixation pathways in prokaryotes",
"2-oxocarboxylic acid metabolism", "pentose phosphate pathway", "sulfur metabolism")
currentMtab <- lauren_mtabs[mtabIndex]
curMtabTable <- keggTable[grep(currentMtab, keggTable$paths, ignore.case = T),]
split(curMtabTable, f = curMtabTable$Matched.Compound)
split(curMtabTable, f = curMtabTable$Matched.Compound)
temp <- split(curMtabTable, f = curMtabTable$Matched.Compound)
temp[[5]]
?all.equal
temp[[5]]$Query.Mass[1]/10^6 * temp[[5]]$ppm[1]
errorAllowed <- temp[[5]]$Query.Mass[1]/10^6 * temp[[5]]$ppm[1]
sapply(featureTable$mz, all.equal, temp[[5]]$Query.Mass[1], tolerance = errorAllowed) == TRUE
sapply(featureTable$mz, all.equal, temp[[5]]$Query.Mass[1], tolerance = errorAllowed) == TRUE %>% any
(sapply(featureTable$mz, all.equal, temp[[5]]$Query.Mass[1], tolerance = errorAllowed) == TRUE) %>% any
featureTableCheck <- sapply(featureTable$mz, all.equal,
temp[[5]]$Query.Mass[1], tolerance = errorAllowed) == TRUE
featureTableCheck
temp[featureTableCheck,]
featureTableCheck
featureTableCheck <- sapply(featureTable$mz, all.equal,
temp[[5]]$Query.Mass[1], tolerance = errorAllowed) == TRUE
temp[featureTableCheck,]
featureTable[featureTableCheck,]
featureTable$rt[featureTableCheck]
observedTimes <- featureTable$rt[featureTableCheck]
observedTimes
order(observedTimes)
sort(observedTimes)
diff(sort(observedTimes)) < 30
diff(sort(observedTimes)) < 15
diff(sort(observedTimes)) < 10
diff(sort(observedTimes)) < 5
rle(diff(sort(observedTimes)) < 5)
observedTimes
kmeans(observedTimes,3)
<- rle(diff(sort(observedTimes)) < 5)
rle(diff(sort(observedTimes)) < 5)
kmeans(observedTimes,5)
pkgs <- c("factoextra",  "NbClust")
install.packages(pkgs)
library(factoextra)
library(NbClust)
fviz_nbclust(observedTimes, FUNcluster, method = c("silhouette", "wss"))
fviz_nbclust(observedTimes, FUNcluster, method = c("silhouette"))
?NbClust
fviz_nbclust(NbClust(observedTimes), FUNcluster, method = c("silhouette"))
NbClust(observedTimes, method = "silhouette")
NbClust(observedTimes, method = "kmeans")
fviz_nbclust(NbClust(observedTimes, method = "kmeans"), FUNcluster, method = c("silhouette"))
NbClust(observedTimes, method = "kmeans")
temp2 <- NbClust(observedTimes, method = "kmeans")
temp2$All.index
temp2$Best.partition
temp2$All.CriticalValues
temp2$Best.partition
table(temp2$Best.partition)
temp2 <- NbClust(observedTimes, method = "kmeans")$Best.partition
temp2
temp2 <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition)
temp2 <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition) %>% max()
temp2
temp2 <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition)
temp2
table(clusterCount) %>% max()
clusterCount <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition)
clusterCount <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition)
table(clusterCount) %>% max()
maxCount <- table(clusterCount) %>% max()
clusterCount <- suppressWarnings(NbClust(observedTimes, method = "kmeans")$Best.partition) %>% table()
maxCount <- clusterCount %>% max()
clusterCount == maxCount
clusterCount[clusterCount == maxCount]
names(clusterCount[clusterCount == maxCount])
names(clusterCount[clusterCount == maxCount]) %>%
as.integer()
clusterNumber <- names(clusterCount[clusterCount == maxCount]) %>%
as.integer() %>%
max()
<- names(clusterCount[clusterCount == maxCount]) %>%
as.integer() %>%
max()
clusterNumber
observedTimes
NbClust(observedTimes, method = "kmeans")
temp2
temp2 <- NbClust(observedTimes, method = "kmeans")
temp2$All.index
temp2$All.CriticalValues
temp2$Best.nc
temp2$Best.partition
clusterNumber
kmeans(observedTimes, clusterNumber)
temp2 <- kmeans(observedTimes, clusterNumber)
temp2
temp2$centers
as.vector(temp2$centers)
rtWindows <- kmeans(observedTimes, clusterNumber)$centers %>% as.vector()
rtWindows
Query.Mass[1]
featureTable$rt
rtIndex <- featureTable$rt > (curRt - rtRange) & featureTable$rt < (curRt + rtRange)
rtIndex <- 1
curRt <- rtWindows[rtIndex]
rtIndex <- featureTable$rt > (curRt - rtRange) & featureTable$rt < (curRt + rtRange)
rtRange <- 30
rtIndex <- featureTable$rt > (curRt - rtRange) & featureTable$rt < (curRt + rtRange)
rtIndex
featureTable$mz[rtIndex]
checkMasses <- featureTable$mz[rtIndex]
checkMasses
rtIntervalMasses <- featureTable$mz[rtIndex]
temp[[5]]$Query.Mass
order(observedTimes)
timeOrder <- order(observedTimes)
observedTimes[timeOrder]
rle(diff(observedTimes[timeOrder]) < 15)
rtGroups <- rle(diff(observedTimes[timeOrder]) < 15)
rtRle <- rle(diff(observedTimes[timeOrder]) < 15)
## method to determine ideal number of cluster to check for within data
observedTimes <- featureTable$rt[featureTableCheck]
timeOrder <- order(observedTimes)
rtRle <- rle(diff(observedTimes[timeOrder]) < 15)
groups <- vector("numeric")
start <- 1
groupCounter <- 1
for(i in 1:length(rtRle$lengths)) {
if(isTRUE(rtRle$values[i])) {
end <- start + rtRle$lengths[i]
groupCounter <- groupCounter + 1
groups <- c(groups, start:end)
start <- end
} else {
start <- start + rtRle$lengths[i]
}
}
groups
groupCounter
groups
start:end
1:length(rtRle$lengths)
groups <- vector("numeric")
start <- 1
groupCounter <- 1
rtRle$lengths
rtRle$values[i]
groups <- vector("numeric")
start <- 1
groupCounter <- 1
groups <- list()
start <- 1
groupCounter <- 1
start
isTRUE(rtRle$values[i])
rtRle$lengths[i]
start
start:end
start
end <- start + rtRle$lengths[i]
end
start:end
## method to determine ideal number of cluster to check for within data
observedTimes <- featureTable$rt[featureTableCheck]
timeOrder <- order(observedTimes)
rtRle <- rle(diff(observedTimes[timeOrder]) < 15)
groups <- list()
start <- 1
groupCounter <- 1
for(i in 1:length(rtRle$lengths)) {
if(isTRUE(rtRle$values[i])) {
end <- start + rtRle$lengths[i]
groups[[groupCounter]] <- start:end
groupCounter <- groupCounter + 1
start <- end
} else {
start <- start + rtRle$lengths[i]
}
}
groups
observedTimes[timeOrder]
lapply(groups, function(curGroup) {
mean(observedTimes[timeOrder][curGroup])
})
checkRts <= sapply(groups, function(curGroup) {
mean(observedTimes[timeOrder][curGroup])
})
checkRts <- sapply(groups, function(curGroup) {
mean(observedTimes[timeOrder][curGroup])
})
checkRts
## Determine retention time windows worth checking out to find peak matches in the data
observedTimes <- featureTable$rt[featureTableCheck]
timeOrder <- order(observedTimes)
rtRle <- rle(diff(observedTimes[timeOrder]) < 15)
groups <- list()
start <- 1
groupCounter <- 1
for(i in 1:length(rtRle$lengths)) {
if(isTRUE(rtRle$values[i])) {
end <- start + rtRle$lengths[i]
groups[[groupCounter]] <- start:end
groupCounter <- groupCounter + 1
start <- end
} else {
start <- start + rtRle$lengths[i]
}
}
checkRts <- sapply(groups, function(curGroup) {
mean(observedTimes[timeOrder][curGroup])
})
checkRts
curRt <- checkRts[rtIndex]
curRt
checkRts
rtIndex
rtIndex <- 1
curRt <- checkRts[rtIndex]
rtRange
## Determine retention time windows worth checking out to find peak matches in the data
rtRange <- 15
rtSub <- featureTable$rt > (curRt - rtRange) & featureTable$rt < (curRt + rtRange)
rtSub
rtIntervalMasses <- featureTable$mz[rtSub]
rtIntervalMasses
rtIntervalMasses
## locating retention times of each compound
temp[[5]]$absError <- temp[[5]]$Query.Mass/ 10^6 * temp[[5]]$ppm
temp[[5]]
errorAllowed <- temp[[5]]$absError[2]
featureTableCheck <- sapply(rtIntervalMasses, all.equal,
temp[[5]]$Query.Mass[2], tolerance = errorAllowed) == TRUE
featureTableCheck
